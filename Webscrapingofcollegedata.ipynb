{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "ZrfwKfIMKKQY"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Wb81TAYjKQ6Z"
   },
   "outputs": [],
   "source": [
    "base_url = 'https://www.4icu.org/in/universities/'\n",
    "req = requests.get(base_url)\n",
    "soup = BeautifulSoup(req.content, 'html5lib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cV6AuLvFSpPe"
   },
   "source": [
    "Defining a function named 'state_url' which takes base website url as argument and returns the list of all the urls of the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "e6hC2xoxNZ7t"
   },
   "outputs": [],
   "source": [
    "def state_url(base_url):\n",
    "  state_content = soup.find_all('div', class_ = 'panel-body')\n",
    "  base_url_ = 'https://www.4icu.org'\n",
    "  list_of_state_urls = []\n",
    "  for each in state_content:\n",
    "    s = each.find_all('tr')\n",
    "    for each1 in s:\n",
    "         a = each1.find_all('a')\n",
    "         for each2 in a:\n",
    "           url_ = each2.get('href') \n",
    "           url_ = base_url_ + url_\n",
    "           list_of_state_urls.append(url_)\n",
    "  return list_of_state_urls\n",
    "State_urls = state_url(base_url)\n",
    "#State_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-apF-umTEFM"
   },
   "source": [
    "Defining a function named 'coll_urls' which take state url as argument and returns the list of all the college urls in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "GVyfEyOKN-iD"
   },
   "outputs": [],
   "source": [
    "def coll_urls(state_url):\n",
    "  base_url_ = 'https://www.4icu.org'\n",
    "  req = requests.get(state_url)\n",
    "  soup = BeautifulSoup(req.content, 'html5lib')\n",
    "  url_content = soup.find_all('div', class_ = 'panel-body')\n",
    "  list_of_college_urls = []\n",
    "  for each in url_content:\n",
    "    s = each.find_all('tr')\n",
    "    for each1 in s:\n",
    "         a = each1.find_all('a')\n",
    "         for each2 in a:\n",
    "           url_ = each2.get('href')\n",
    "           url_ = base_url_ + url_\n",
    "           list_of_college_urls.append(url_)\n",
    "  return list_of_college_urls[7:-1]\n",
    "State_wise_coll_urls =coll_urls('https://www.4icu.org/in/kerala/')  \n",
    "#State_wise_coll_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "-g5v_NPLi6jA"
   },
   "outputs": [],
   "source": [
    "capital_cities = ['Amaravati', 'Itanagar', 'Dispur', 'Patna', '\tRaipur', 'Panaji',\n",
    "                 'Gandhinagar', 'Chandigarh', 'Shimla', 'Srinagar', 'Ranchi', 'Bengaluru',\n",
    "                 'Trivandrum','Bhopal', 'Mumbai', 'Imphal', 'Shillong', 'Aizawl', 'Kohima',\n",
    "                 'Bhubaneswar', 'Chandigarh', 'Jaipur', 'Gangtok', 'Chennai', 'Hyderabad',\n",
    "                 'Agartala', 'Lucknow', 'Dehradun', 'Kolkata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "rxMMslq6j6pi"
   },
   "outputs": [],
   "source": [
    "def capital_cities_colleges_url(url):\n",
    "    base_url_ = 'https://www.4icu.org'\n",
    "    req = requests.get(state_url)\n",
    "    soup = BeautifulSoup(req.content, 'html5lib')\n",
    "    url_content = soup.find_all('div', class_ = 'panel-body')\n",
    "    list = []\n",
    "    for each in url_content:\n",
    "      s = each.find_all('tr')\n",
    "      for each1 in s:\n",
    "          a = each1.find_all('a')\n",
    "          td = each1.find_all('td')\n",
    "          if len(td) == 3:\n",
    "            town = td[-1].text\n",
    "            if town in capital_cities:\n",
    "               for each2 in a:\n",
    "                    url_ = each2.get('href')\n",
    "                    url_ = base_url_ + url_\n",
    "                    list.append(url_)\n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "yIh2V0qYsLLO"
   },
   "outputs": [],
   "source": [
    "metro_cities = ['Mumbai', 'Delhi', 'Kolkata', 'Lucknow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "p_aa1cMOtImc"
   },
   "outputs": [],
   "source": [
    "def metro_cities_colleges_url(url):\n",
    "    base_url_ = 'https://www.4icu.org'\n",
    "    req = requests.get(url)\n",
    "    soup = BeautifulSoup(req.content, 'html5lib')\n",
    "    url_content = soup.find_all('div', class_ = 'panel-body')\n",
    "    list = []\n",
    "    for each in url_content:\n",
    "      s = each.find_all('tr')\n",
    "      for each1 in s:\n",
    "          a = each1.find_all('a')\n",
    "          td = each1.find_all('td')\n",
    "          if len(td) == 3:\n",
    "            town = td[-1].text\n",
    "            if town in metro_cities:\n",
    "               for each2 in a:\n",
    "                    url_ = each2.get('href')\n",
    "                    url_ = base_url_ + url_\n",
    "                    list.append(url_)\n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "fYWPeJPIu8PL"
   },
   "outputs": [],
   "source": [
    "\n",
    "def college_details(url):\n",
    "  req = requests.get(url)\n",
    "  soup = BeautifulSoup(req.content, 'html5lib')\n",
    "\n",
    "  dict_ = {}\n",
    "\n",
    "  # College name\n",
    "  name_ = soup.find('ol', class_ = 'breadcrumb small')\n",
    "  dict_['College Name'] = name_.find_all('span', itemprop = 'name')[-1].text\n",
    "\n",
    "  # College url\n",
    "  url_ = soup.find_all('a', itemprop=\"url\")\n",
    "  dict_['College url'] = url_[1].get('href')\n",
    "\n",
    "  # College Address\n",
    "  add_1 = soup.find_all('span', itemprop = 'streetAddress')[0].text\n",
    "  add_2 = soup.find_all('span', itemprop = 'addressLocality')[0].text\n",
    "  add_3 = soup.find_all('span', itemprop = 'postalCode')[0].text\n",
    "  add_4 = soup.find_all('span', itemprop = 'addressRegion')[0].text\n",
    "  add = add_1 + ' ' + add_2 + add_3 + ' ' + add_4\n",
    "  dict_['College Address'] = add\n",
    "\n",
    "  # College Contact Number\n",
    "  contact_no = soup.find_all('span', itemprop = 'telephone')[0].text\n",
    "  dict_['Contact no.'] = contact_no\n",
    "\n",
    "  return dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sjwrJYBEuu5r",
    "outputId": "1875f7b6-67a0-42b5-c64c-a3c89756b2a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter 1, for the details of all the top colleges in India\n",
      "Enter 2, for all the colleges in capital cities of India\n",
      "Enter 3, for all the colleges in metro cities of India\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print('Enter 1, for the details of all the top colleges in India')\n",
    "print('Enter 2, for all the colleges in capital cities of India')\n",
    "print('Enter 3, for all the colleges in metro cities of India')\n",
    "\n",
    "input_ = int(input())\n",
    "if input_ == 1:\n",
    "  colleges_list = []\n",
    "  for each in State_urls:\n",
    "      colleges = coll_urls(each)\n",
    "      for each1 in colleges:\n",
    "         colleges_list.append(college_details(each1))\n",
    "  df = pd.DataFrame(colleges_list)\n",
    "  df.to_csv('all_colleges.csv')\n",
    "elif input_ == 2:\n",
    "  colleges_list = []\n",
    "  for each in State_urls:\n",
    "      colleges = capital_cities_colleges_url(each)\n",
    "      for each1 in colleges:\n",
    "         colleges_list.append(college_details(each1))\n",
    "  df = pd.DataFrame(colleges_list)\n",
    "  df.to_csv('capital_colleges.csv')\n",
    "elif input_ == 3:\n",
    "  colleges_list = []\n",
    "  for each in State_urls:\n",
    "      colleges = metro_cities_colleges_url(each)\n",
    "      for each1 in colleges:\n",
    "         colleges_list.append(college_details(each1))\n",
    "  df = pd.DataFrame(colleges_list)\n",
    "  df.to_csv('metrocities_colleges.csv')\n",
    "else:\n",
    "  print('Invalid Input')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
